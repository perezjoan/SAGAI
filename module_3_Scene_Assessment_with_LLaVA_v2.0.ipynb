{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1N2vNjuZeDCmIXQFF1xcrTTQ4I5kvJ9P-","timestamp":1744900479926}],"gpuType":"T4","authorship_tag":"ABX9TyNsn1CvZi7cqYfRhwRONHLR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# MODULE 3: SCENE ASSESSMENT WITH LLaVA\n","# Version: v2.0 (SAGAI v1.1)\n","# Git repo: https://github.com/perezjoan/SAGAI\n","# This script performs batch visual scoring of street-level images using\n","# LLaVA-Next (v1.6 Mistral) loaded directly from Hugging Face. The processor\n","# handles image preprocessing, tokenization, and chat formatting, while the\n","# model generates a concise numeric response based on a task-specific prompt.\n","# Users define a structured prompt composed of four elementsâ€”role description,\n","# theory or scoring guide, task instruction, and required output format.\n","# The model then applies this rubric consistently to each image in the input\n","# folder. The script loops through all valid images stored in Google Drive,\n","# extracts the modelâ€™s numeric output, and saves results into a resumable CSV\n","# file.\n","#This makes it well suited for large-scale, zero-shot visual assessment\n","# tasks such as classification, counting, and spatial measurement."],"metadata":{"id":"jJtOE2cL5jrN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# BLOCK 1 â€” Install dependencies + Hugging Face login\n","# ===============================================\n","\n","!pip install --no-cache-dir -q transformers accelerate bitsandbytes\n","\n","from google.colab import userdata\n","from huggingface_hub import login\n","\n","# Load token stored in Colab Secrets (name: HF_TOKEN)\n","HF_TOKEN = userdata.get('HF_TOKEN')\n","\n","if HF_TOKEN is None:\n","    raise ValueError(\"âŒ HF_TOKEN not found in Colab secrets. Add it in 'Settings â†’ Notebook secrets'\")\n","else:\n","    print(\"âœ… HuggingFace token loaded:\", HF_TOKEN[:10] + \"...\")\n","\n","login(token=HF_TOKEN)\n"],"metadata":{"id":"Qlmpeoc9kr-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# BLOCK 2 â€” Mount Google Drive + Define Paths\n","# ===============================================\n","\n","from google.colab import drive\n","import os\n","\n","# Mount Drive\n","drive.mount('/content/drive')\n","\n","# User parameters\n","case_study = \"nice\"         # e.g., \"vienna\", \"nice\"\n","selected_task = \"T1\"        # Options: T1 = CATEGORIZATION, T2 = COUNTING, T3 = MEASURING\n","display_images = True # Display images for visual verification (optional)\n","\n","# Folder structure\n","root_path = f\"/content/drive/MyDrive/SAGAI\"\n","image_folder = os.path.join(root_path, f\"StreetViewBatchDownload_{case_study.capitalize()}\")\n","output_path = os.path.join(root_path, f\"Image_Analysis/Score_Analysis_LLaVA_{case_study.capitalize()}_{selected_task}.csv\")\n","\n","# Ensure output directory exists\n","os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","\n","print(\"ğŸ“ Image folder:\", image_folder)\n","print(\"ğŸ’¾ Output file:\", output_path)\n"],"metadata":{"id":"_39AivU25qlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCY3GXB3fLwr"},"outputs":[],"source":["# ===============================================\n","# BLOCK 3 â€” Load LLaVA v1.6 Mistral (HuggingFace)\n","# ===============================================\n","\n","from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n","import torch\n","\n","# Model ID â€” change if needed\n","model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n","\n","# Load processor (tokenizer + image processor + chat template)\n","processor = LlavaNextProcessor.from_pretrained(\n","    model_id,\n","    token=HF_TOKEN\n",")\n","\n","# Load model (4-bit quantization to fit GPU memory)\n","model = LlavaNextForConditionalGeneration.from_pretrained(\n","    model_id,\n","    token=HF_TOKEN,\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=True,\n","    load_in_4bit=True\n",").to(\"cuda\")\n","\n","print(\"ğŸ”¥ LLaVA v1.6 Mistral loaded successfully!\")\n"]},{"cell_type":"code","source":["# ===============================================\n","# BLOCK 4 â€” SAGAI Image Analysis Function (LLaVA-Next)\n","# ===============================================\n","\n","from PIL import Image\n","from io import BytesIO\n","import requests\n","import re\n","\n","def caption_image(\n","    image_file,\n","    prompt,\n","    do_sample=True,\n","    temperature=0.3,\n","    top_p=0.9,\n","    max_new_tokens=10\n","):\n","    \"\"\"\n","    Sends an image + prompt to LLaVA-Next and returns ONLY the numeric score,\n","    based on the task prompt provided in Block 5.\n","    \"\"\"\n","\n","    # -------------------------\n","    # Load image (local or URL)\n","    # -------------------------\n","    if isinstance(image_file, str) and image_file.startswith(\"http\"):\n","        image = Image.open(BytesIO(requests.get(image_file).content)).convert(\"RGB\")\n","    else:\n","        image = Image.open(image_file).convert(\"RGB\")\n","\n","    # ----------------------------------------------------\n","    # Build conversation structure required by LLaVA-Next\n","    # ----------------------------------------------------\n","    conversation = [\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image\"},                 # placeholder indicating an image is included\n","                {\"type\": \"text\", \"text\": prompt},  # the task prompt (T1/T2/T3)\n","            ],\n","        }\n","    ]\n","\n","    # -------------------------------------------------------\n","    # Format the conversation into a ready-to-tokenize string\n","    # (using the model's built-in chat template)\n","    # -------------------------------------------------------\n","    prompt_string = processor.apply_chat_template(\n","        conversation,\n","        add_generation_prompt=True,  # instruct model to prepare for generation\n","        tokenize=False               # return a string, not tokens\n","    )\n","\n","    # -----------------------------------------------------\n","    # Convert text + image into tensors expected by model\n","    # -----------------------------------------------------\n","    inputs = processor(\n","        images=image,\n","        text=prompt_string,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Move tensors to GPU\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","\n","    # ------------------------\n","    # Generate model response\n","    # ------------------------\n","    output = model.generate(\n","        **inputs,\n","        do_sample=do_sample,\n","        temperature=temperature,\n","        top_p=top_p,\n","        max_new_tokens=max_new_tokens\n","    )\n","\n","    # -------------------------\n","    # Decode generated text\n","    # -------------------------\n","    raw = processor.decode(output[0], skip_special_tokens=True).strip()\n","\n","    # ------------------------------------------------------------------\n","    # Remove the instruction prefix ([INST] ... [/INST]) if present\n","    # LLaVA-Next outputs may include the full prompt before the answer.\n","    # ------------------------------------------------------------------\n","    if \"[/INST]\" in raw:\n","        raw = raw.split(\"[/INST]\")[-1].strip()\n","\n","    # ----------------------------------------------------------\n","    # Extract ONLY numeric content (0/1/2 or width like \"1.5\")\n","    # ----------------------------------------------------------\n","    numbers = re.findall(r\"-?\\d+(?:\\.\\d+)?\", raw)\n","\n","    # If at least one number was found â†’ return the last one\n","    if len(numbers) > 0:\n","        return numbers[-1]\n","\n","    # If model output contains no numeric value â†’ return NA\n","    return \"NA\"\n"],"metadata":{"id":"1NTF3yO86w-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# BLOCK 5 â€” Build task-specific prompt\n","# ===============================================\n","\n","if selected_task == \"T1\":  # CATEGORIZATION\n","    role_description = (\n","        \"You are an AI trained to visually analyze street-level images. \"\n","        \"Your task is to determine whether the environment shown is urban or rural.\"\n","    )\n","    theory_model = (\n","        \"Classification Guide:\\n\"\n","        \"- 0 = Rural area: sparse built environment or natural landscapes.\\n\"\n","        \"- 1 = Urban area: dense built environment or visible infrastructure.\"\n","    )\n","    task = \"Return ONLY 0 or 1.\"\n","    response_format = \"Answer format: 0 or 1\"\n","\n","elif selected_task == \"T2\":  # COUNTING\n","    role_description = (\n","        \"You are an AI trained to visually detect commercial storefronts.\"\n","    )\n","    theory_model = (\n","        \"Scoring Guide:\\n\"\n","        \"- 0 = No visible shops\\n\"\n","        \"- 1 = One visible shop\\n\"\n","        \"- 2 = More than one shop\"\n","    )\n","    task = \"Return ONLY 0, 1, or 2.\"\n","    response_format = \"Answer format: 0, 1, or 2\"\n","\n","elif selected_task == \"T3\":  # MEASURING\n","    role_description = (\n","        \"You are an AI trained to estimate sidewalk width from street-level images.\"\n","    )\n","    theory_model = (\n","        \"Scoring Guide:\\n\"\n","        \"- 0 = No visible sidewalk\\n\"\n","        \"- Otherwise return sidewalk width rounded to nearest 0.5 meters.\"\n","    )\n","    task = \"Return ONLY one number (0, 1.0, 1.5, 2.0, etc.).\"\n","    response_format = \"Answer format: 0 or a number like 1.5\"\n","\n","else:\n","    raise ValueError(\"Invalid selected_task: choose T1, T2, or T3\")\n","\n","# Final prompt\n","prompt = f\"\"\"\n","{role_description}\n","\n","{theory_model}\n","\n","{task}\n","\n","{response_format}\n","\"\"\"\n","\n","print(\"Prompt built for task:\", selected_task)\n","print(prompt)\n","\n","# ===============================================\n","# GLOBAL SAMPLING SETTINGS (used for ALL tasks)\n","# ===============================================\n","do_sample = True # Enable sampling (adds variation)\n","temperature = 0.3 # Low temperature for controlled creativity\n","top_p = 0.9 # Nucleus sampling within top 90% probable tokens\n","max_new_tokens = 10 # Expecting short numeric response\n"],"metadata":{"id":"palJ8Pwr9FEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================================\n","# BLOCK 6 â€” Run SAGAI Scoring on Image Folder\n","# ===============================================\n","\n","import csv\n","import time\n","import pandas as pd\n","from IPython.display import display\n","\n","# Start timer\n","start_time = time.time()\n","\n","# ------------------ Load already processed images ------------------\n","already_processed = set()\n","\n","if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n","    df_existing = pd.read_csv(output_path)\n","    already_processed = set(df_existing[\"image_name\"].tolist())\n","    print(f\"ğŸ” Resuming previous run â€” {len(already_processed)} images already scored.\")\n","else:\n","    print(\"ğŸ†• Starting fresh â€” no previous CSV found.\")\n","\n","# ------------------ Open CSV (append mode) ------------------\n","with open(output_path, \"a\", newline=\"\") as csvfile:\n","    writer = csv.writer(csvfile)\n","\n","    # Write header only if file is empty\n","    if os.path.getsize(output_path) == 0:\n","        writer.writerow([\"image_name\", \"score\"])\n","\n","    # Iterate through images\n","    for fname in sorted(os.listdir(image_folder)):\n","        if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n","            continue\n","\n","        if fname in already_processed:\n","            continue  # Skip already processed\n","\n","        image_path = os.path.join(image_folder, fname)\n","\n","        # Handle files marked as missing (from your GSV downloader)\n","        if \"_NA\" in fname:\n","            print(f\"âš ï¸ Skipping NA image: {fname}\")\n","            writer.writerow([fname, \"NA\"])\n","            continue\n","\n","        print(f\"\\nğŸ” Processing: {fname}\")\n","\n","        try:\n","            # Run inference\n","            result = caption_image(\n","                image_path,\n","                prompt,\n","                do_sample,\n","                temperature,\n","                top_p,\n","                max_new_tokens\n","            )\n","\n","            print(f\"ğŸ“ Score: {result}\")\n","\n","            # Optional display\n","            if display_images:\n","                display(Image.open(image_path))\n","\n","            # Save result\n","            writer.writerow([fname, result.strip()])\n","\n","        except Exception as e:\n","            print(f\"âŒ Error on {fname}: {e}\")\n","            writer.writerow([fname, f\"ERROR: {e}\"])\n","\n","# ------------------ Summary ------------------\n","elapsed_time = time.time() - start_time\n","\n","print(\"\\nâœ… SAGAI scoring completed.\")\n","print(f\"ğŸ“„ Results saved to: {output_path}\")\n","print(f\"â±ï¸ Total processing time: {elapsed_time:.2f} seconds\")\n"],"metadata":{"id":"-3fMBya6vaPN"},"execution_count":null,"outputs":[]}]}